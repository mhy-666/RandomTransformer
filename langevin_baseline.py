
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Langevin Dynamics Sampling Baseline for Random Backpropagation
åœ¨æ ‡å‡†è®­ç»ƒä¸­ç›´æ¥å¯¹æ¢¯åº¦æ·»åŠ å™ªå£°ï¼ˆSGLDé£æ ¼ï¼‰
"""

import torch
import torch.nn as nn
from typing import Dict, Optional
import numpy as np


class LangevinNoiseManager:
    """ç®¡ç†LangevinåŠ¨åŠ›å­¦å™ªå£°æ³¨å…¥"""

    def __init__(
        self,
        model: nn.Module,
        noise_scale: float = 0.01,
        apply_to_layers: str = 'all',  # 'all', 'embedding', 'attn', 'mlp'
        temperature: float = 1.0,
        use_preconditioner: bool = False,
        random_seed: Optional[int] = None,
        device: str = 'cuda'
    ):
        """
        Args:
            noise_scale: å™ªå£°å¼ºåº¦ï¼ˆç›¸å½“äºSGLDä¸­çš„å­¦ä¹ ç‡ï¼‰
            apply_to_layers: å¯¹å“ªäº›å±‚åº”ç”¨å™ªå£°
            temperature: æ¸©åº¦å‚æ•°ï¼ˆæ§åˆ¶æ¢ç´¢vsåˆ©ç”¨ï¼‰
            use_preconditioner: æ˜¯å¦ä½¿ç”¨é¢„æ¡ä»¶ï¼ˆç±»ä¼¼RMSpropï¼‰
            random_seed: éšæœºç§å­
            device: è®¾å¤‡
        """
        self.model = model
        self.noise_scale = noise_scale
        self.apply_to_layers = apply_to_layers
        self.temperature = temperature
        self.use_preconditioner = use_preconditioner
        self.device = device


        if random_seed is not None:
            self.rng = torch.Generator(device=torch.device(device))
            self.rng.manual_seed(random_seed)
        else:
            self.rng = None


        # çŠ¶æ€æ ‡å¿—
        self.is_enabled = True

        # é¢„æ¡ä»¶çŸ©é˜µï¼ˆå¦‚æœä½¿ç”¨ï¼‰
        self.precond_dict: Dict[str, torch.Tensor] = {}
        if use_preconditioner:
            self._initialize_preconditioner()

        # æ³¨å†Œhooks
        self.hooks = []
        self._register_hooks()

        print("=" * 70)
        print("Langevin Dynamics Noise Manager Initialized")
        print(f"  Noise Scale: {noise_scale}")
        print(f"  Temperature: {temperature}")
        print(f"  Apply to: {apply_to_layers}")
        print(f"  Preconditioner: {use_preconditioner}")
        print(f"  Random Seed: {random_seed}")
        print("=" * 70)

    def _should_apply_noise(self, name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦å¯¹è¯¥å‚æ•°åº”ç”¨å™ªå£°"""
        if self.apply_to_layers == 'all':
            return True
        elif self.apply_to_layers == 'embedding':
            return 'wte' in name or 'wpe' in name or 'lm_head' in name
        elif self.apply_to_layers == 'attn':
            return 'attn' in name
        elif self.apply_to_layers == 'mlp':
            return 'mlp' in name
        else:
            return False

    def _initialize_preconditioner(self):
        """åˆå§‹åŒ–é¢„æ¡ä»¶çŸ©é˜µï¼ˆç±»ä¼¼RMSpropçš„ç´¯ç§¯å¹³æ–¹æ¢¯åº¦ï¼‰"""
        for name, param in self.model.named_parameters():
            if param.requires_grad and self._should_apply_noise(name):
                # åˆå§‹åŒ–ä¸ºå°å€¼ï¼Œé¿å…é™¤0
                self.precond_dict[name] = torch.ones_like(param) * 1e-8

    def _update_preconditioner(self, name: str, grad: torch.Tensor, decay: float = 0.99):
        """æ›´æ–°é¢„æ¡ä»¶çŸ©é˜µï¼ˆEMA of squared gradientsï¼‰"""
        if name in self.precond_dict:
            self.precond_dict[name] = (
                decay * self.precond_dict[name] + 
                (1 - decay) * grad.pow(2)
            )

    def _add_langevin_noise(self, grad: torch.Tensor, name: str) -> torch.Tensor:
        """
        æ·»åŠ Langevinå™ªå£°åˆ°æ¢¯åº¦

        SGLDæ›´æ–°: Î¸_t+1 = Î¸_t - Î·âˆ‡L + âˆš(2Î·Â·T)Â·Îµ
        ç­‰ä»·äºåœ¨æ¢¯åº¦ä¸Š: grad = âˆ‡L - âˆš(2Î·Â·T)/Î·Â·Îµ = âˆ‡L - âˆš(2T/Î·)Â·Îµ
        """
        if not self.is_enabled:
            return grad

        # ç”Ÿæˆé«˜æ–¯å™ªå£°
        if self.rng is not None:
            # ä½¿ç”¨å›ºå®šçš„ generator (å¯å¤ç°)
            noise = torch.randn(
                grad.shape,
                dtype=grad.dtype,
                device=grad.device,
                generator=self.rng
            )
        else:
            # ä½¿ç”¨é»˜è®¤éšæœºæ•°ç”Ÿæˆå™¨
            noise = torch.randn_like(grad)

        # åº”ç”¨é¢„æ¡ä»¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_preconditioner and name in self.precond_dict:
            # æ›´æ–°é¢„æ¡ä»¶çŸ©é˜µ
            self._update_preconditioner(name, grad)

            # é¢„æ¡ä»¶å™ªå£°: G^(-1/2) * noise
            precond = self.precond_dict[name]
            noise = noise / (torch.sqrt(precond) + 1e-8)

        # è®¡ç®—å™ªå£°æ ‡å‡†å·®: Ïƒ = âˆš(2Â·temperatureÂ·noise_scale)
        noise_std = np.sqrt(2.0 * self.temperature * self.noise_scale)
        # noise_std = np.sqrt(2 * self.temperature * self.lr)
        # æ·»åŠ å™ªå£°åˆ°æ¢¯åº¦
        noisy_grad = grad + noise_std * noise

        return noisy_grad

    def _register_hooks(self):
        """æ³¨å†Œbackward hooksæ¥æ³¨å…¥å™ªå£°"""
        for name, param in self.model.named_parameters():
            if param.requires_grad and self._should_apply_noise(name):
                # æ³¨å†Œhook
                hook = param.register_hook(
                    lambda grad, n=name: self._add_langevin_noise(grad, n)
                )
                self.hooks.append(hook)
                print(f"  Registered Langevin hook: {name:60s}")

    def disable(self):
        """ç¦ç”¨Langevinå™ªå£°"""
        print("\n" + "=" * 70)
        print("DISABLING LANGEVIN NOISE - Switching to Standard Training")
        print("=" * 70)
        self.is_enabled = False

    def enable(self):
        """é‡æ–°å¯ç”¨Langevinå™ªå£°"""
        print("\n" + "=" * 70)
        print("RE-ENABLING LANGEVIN NOISE")
        print("=" * 70)
        self.is_enabled = True

    def remove_hooks(self):
        """ç§»é™¤æ‰€æœ‰hooks"""
        for hook in self.hooks:
            hook.remove()
        self.hooks.clear()

    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'is_enabled': self.is_enabled,
            'noise_scale': self.noise_scale,
            'temperature': self.temperature,
            'apply_to_layers': self.apply_to_layers,
            'use_preconditioner': self.use_preconditioner,
            'num_hooks': len(self.hooks)
        }
        return stats


# ============ ä½¿ç”¨ç¤ºä¾‹ï¼ˆé›†æˆåˆ°ä½ çš„experiment_llm-2.pyä¸­ï¼‰============

def setup_langevin_baseline(
    model,
    noise_scale: float = 0.01,
    apply_to_layers: str = 'embedding',  # åªå¯¹embeddingåŠ å™ªå£°
    temperature: float = 1.0,
    use_preconditioner: bool = False,
    random_seed: Optional[int] = None,
    device: str = 'cuda'
):
    """
    è®¾ç½®Langevin Dynamics Baseline

    ä½¿ç”¨ç¤ºä¾‹:
        langevin_manager = setup_langevin_baseline(
            model=model,
            noise_scale=0.01,
            apply_to_layers='embedding',
            temperature=1.0
        )
    """
    manager = LangevinNoiseManager(
        model=model,
        noise_scale=noise_scale,
        apply_to_layers=apply_to_layers,
        temperature=temperature,
        use_preconditioner=use_preconditioner,
        random_seed=random_seed,
        device=device
    )
    return manager


# ============ Callback: è®­ç»ƒä¸­é€”å…³é—­Langevinå™ªå£° ============

from transformers.trainer_callback import TrainerCallback
import wandb

class DisableLangevinCallback(TrainerCallback):
    """åœ¨è®­ç»ƒç‰¹å®šæ­¥æ•°åå…³é—­Langevinå™ªå£°çš„å›è°ƒ"""

    def __init__(self, langevin_manager, disable_ratio, max_steps):
        """
        Args:
            langevin_manager: LangevinNoiseManagerå®ä¾‹
            disable_ratio: åœ¨æ€»æ­¥æ•°çš„è¿™ä¸ªæ¯”ä¾‹åå…³é—­ (0.9 = 90%)
            max_steps: æ€»è®­ç»ƒæ­¥æ•°
        """
        self.langevin_manager = langevin_manager
        self.disable_step = int(max_steps * disable_ratio)
        self.disabled = False

        print("=" * 70)
        print(f"DisableLangevinCallback Initialized")
        print(f"  Will disable at step: {self.disable_step} / {max_steps} ({disable_ratio*100:.0f}%)")
        print("=" * 70)

    def on_step_begin(self, args, state, control, **kwargs):
        """åœ¨æ¯ä¸ªè®­ç»ƒæ­¥å¼€å§‹æ—¶æ£€æŸ¥æ˜¯å¦éœ€è¦å…³é—­Langevinå™ªå£°"""
        if not self.disabled and state.global_step >= self.disable_step:
            print("\n" + "=" * 70)
            print(f"ğŸ”„ TRAINING MILESTONE: Step {state.global_step}/{args.max_steps}")
            print(f"   Disabling Langevin Noise")
            print(f"   Switching to Standard Gradient Descent")
            print("=" * 70 + "\n")

            self.langevin_manager.disable()
            self.disabled = True

            # è®°å½•åˆ°wandb
            wandb.log({
                'langevin_noise_disabled': True,
                'disable_step': state.global_step,
                'training_phase': 'standard'
            })


# ============ é›†æˆåˆ°è®­ç»ƒæµç¨‹ ============

"""
åœ¨ä½ çš„experiment_llm-2.pyä¸­ä½¿ç”¨:

# 1. å†»ç»“ç­–ç•¥ï¼ˆä¸åŸæ¥ç›¸åŒï¼‰
layers_to_freeze, _ = apply_freeze_strategy(model, args.weight_frozen, 'all')
num_frozen, num_trainable = execute_freeze(model, layers_to_freeze, ['q', 'k', 'v'])

# 2. è®¾ç½®Langevinå™ªå£°ï¼ˆæ›¿ä»£Random Backpropï¼‰
langevin_manager = setup_langevin_baseline(
    model=model,
    noise_scale=args.langevin_noise_scale,      # æ–°å‚æ•°
    apply_to_layers='embedding',                # åªå¯¹embeddingåŠ å™ªå£°
    temperature=args.langevin_temperature,      # æ–°å‚æ•°
    use_preconditioner=args.langevin_precond,   # æ–°å‚æ•°
    random_seed=args.seed,
    device=args.device
)

# 3. è®­ç»ƒé…ç½®
training_args = TrainingArguments(...)

# 4. Callbackè®¾ç½®
callbacks = [EvalCallback()]

if langevin_manager and args.disable_langevin_at_ratio < 1.0:
    disable_callback = DisableLangevinCallback(
        langevin_manager=langevin_manager,
        disable_ratio=args.disable_langevin_at_ratio,
        max_steps=args.max_steps
    )
    callbacks.append(disable_callback)

# 5. è®­ç»ƒ
trainer = Trainer(model=model, args=training_args, callbacks=callbacks, ...)
trainer.train()
"""


# ============ å‘½ä»¤è¡Œå‚æ•°ï¼ˆæ·»åŠ åˆ°argparseï¼‰============

"""
åœ¨experiment_llm-2.pyçš„argparseéƒ¨åˆ†æ·»åŠ :

parser.add_argument('--use_langevin_baseline', action='store_true',
                    help='Use Langevin dynamics noise instead of random backprop')
parser.add_argument('--langevin_noise_scale', type=float, default=0.01,
                    help='Noise scale for Langevin dynamics (Î·)')
parser.add_argument('--langevin_temperature', type=float, default=1.0,
                    help='Temperature parameter for Langevin sampling')
parser.add_argument('--langevin_precond', action='store_true',
                    help='Use preconditioner (RMSprop-style) for Langevin noise')
parser.add_argument('--langevin_apply_to', type=str, default='embedding',
                    choices=['all', 'embedding', 'attn', 'mlp'],
                    help='Which layers to apply Langevin noise')
parser.add_argument('--disable_langevin_at_ratio', type=float, default=1.0,
                    help='Disable Langevin noise after this ratio of training')
"""


# ============ å®éªŒå¯¹æ¯”ç¤ºä¾‹ ============

"""
å®éªŒ1: Random Backpropagation (ä½ çš„åŸæ–¹æ³•)
python experiment_llm-2.py \
    --weight_frozen 1 \
    --random_backprop_strategy full_random \
    --seed 42 \
    --max_steps 10000

å®éªŒ2: Langevin Dynamics Baseline
python experiment_llm-2.py \
    --weight_frozen 1 \
    --use_langevin_baseline \
    --langevin_noise_scale 0.01 \
    --langevin_temperature 1.0 \
    --langevin_apply_to embedding \
    --seed 42 \
    --max_steps 10000

å®éªŒ3: Langevin + Preconditioner
python experiment_llm-2.py \
    --weight_frozen 1 \
    --use_langevin_baseline \
    --langevin_noise_scale 0.01 \
    --langevin_precond \
    --seed 42 \
    --max_steps 10000

å®éªŒ4: ä¸­é€”å…³é—­å™ªå£°
python experiment_llm-2.py \
    --weight_frozen 1 \
    --use_langevin_baseline \
    --langevin_noise_scale 0.01 \
    --disable_langevin_at_ratio 0.9 \
    --seed 42 \
    --max_steps 10000
"""
